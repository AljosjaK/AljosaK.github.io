<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jonathan Ö. Rittmo || Blog</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Jonathan Ö. Rittmo || Blog</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 28 Jun 2018 00:00:00 +0100</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Jonathan Ö. Rittmo || Blog</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Primary Concepts of Power Curves</title>
      <link>/post/primary-concepts-of-power-curves/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/primary-concepts-of-power-curves/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/pymjs/pym.v1.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/widgetframe-binding/widgetframe.js&#34;&gt;&lt;/script&gt;


&lt;!-- This post did not at all become what I thought it would when I started writing it. I was going to write about visualisation of multivariate power curves, but halfway through of explaining what a power curve is I realised that it probably would be better to split the post into two. So here comes the first half! --&gt;
&lt;p&gt;As the image above (probably abused by me) clearly states: “the abuse of power comes as no surprise”. Whether intentional or not, ingorance of a priori power calculations have been prominent in psychology and related fields.&lt;/p&gt;
&lt;p&gt;But this is not surprising, because it is difficult to grasp what a power calculation actually is. In this post I will walk through some of the basic calculations behind deriving the power function for a paired samples z-test analytically (that is obtaining an exact mathematical function) and how you can calculate power for an independent two sample t-test, by simulations in R. Hopefully this will show that the basic concepts of power calculations and power curves actually are quite easy! So here goes:&lt;/p&gt;
&lt;p&gt;The power function for a hypothesis test is defined as the following:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[power(\theta) =\mathbb{P}[\text{reject} \: H_0 \: \text{for a given} \: \theta]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is a given effect size, &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}\)&lt;/span&gt; probability and &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; the null hypothesis. In more general terms it is defined (the vertical bar is read “given”):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[power =\mathbb{P}[\text{reject} \: H_0\:  | \: H_1 \: \text{is true}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Say that we want to test whether a medication has had some effect on a sample (we measure a continuous dependent variable on the same individuals before and after treatment) and we assume that we know the population standard deviation. If &lt;span class=&#34;math inline&#34;&gt;\(\overline{D}\)&lt;/span&gt; is the average difference between the observations, for each individual then &lt;span class=&#34;math inline&#34;&gt;\(\overline{D} \sim N(\mu, \sigma^2)\)&lt;/span&gt;, meaning that &lt;span class=&#34;math inline&#34;&gt;\(\overline{D}\)&lt;/span&gt; follows the normal distribution with some population mean (&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;) and variance (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;). Standardising &lt;span class=&#34;math inline&#34;&gt;\(\overline{D}\)&lt;/span&gt; by centering it around 0 (since we are talking about a difference it is already centered around 0 though) and dividing by the standard error we would have an entitiy that follows the standard normal distribution:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\overline{D}-0}{{{\sigma}_D}/{\sqrt{n}}}\sim Z\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(\overline{D}\)&lt;/span&gt; is the average of the differences between the two measurement time points and &lt;span class=&#34;math inline&#34;&gt;\({\sigma}_D\)&lt;/span&gt; the population standard deviation and hence &lt;span class=&#34;math inline&#34;&gt;\({{\sigma}_D}/{\sqrt{n}}\)&lt;/span&gt; the standard error and thus, if we input real values we would get a z-value. The null and alternative hypotheses for this test are &lt;span class=&#34;math inline&#34;&gt;\(H_0:\: \mu_D \: = \: 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(H_1:\: \mu_D \: \neq \: 0\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\mu_D\)&lt;/span&gt; is estimated by &lt;span class=&#34;math inline&#34;&gt;\(\overline{D}_n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If we set &lt;span class=&#34;math inline&#34;&gt;\(\alpha\:=\:0.05\)&lt;/span&gt; (i.e. &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}\)&lt;/span&gt;[reject &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; | &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; is true]&lt;span class=&#34;math inline&#34;&gt;\(\:=\:0.05\)&lt;/span&gt;) our critical z-value would be &lt;span class=&#34;math inline&#34;&gt;\(z_{(0.05/2) }=\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\pm\)&lt;/span&gt; 1.96. If you are unfamilliar with critical values, have a look at the density plot below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/primary-concepts/index_files/figure-html/zdist-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;All observed z-values that fall above the upper or below the lower critical cutoff values are considered significant at the &lt;span class=&#34;math inline&#34;&gt;\(\alpha \: = \: 0.05\)&lt;/span&gt; level. Now suppose that &lt;span class=&#34;math inline&#34;&gt;\(\mu_D \: \neq \: 0\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\mu_D =\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Then we get the power by calculating:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbb{P}[1.96 &amp;lt; Z &amp;lt; -1.96 \; | \; \mu_D =\theta]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(\frac{\overline{D}_n-0}{{{\sigma}_D}/{\sqrt{n}}}\sim Z\)&lt;/span&gt; we’ll write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbb{P}[1.96 &amp;lt; \frac{\overline{D}_{n}-0}{{{\sigma}_D}/{\sqrt{n}}} &amp;lt; -1.96 \; | \; \mu_D =\theta]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We’ll subtract and add &lt;span class=&#34;math inline&#34;&gt;\(\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}\)&lt;/span&gt;, i.e. a neutral operation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbb{P}[1.96 &amp;lt; \frac{\overline{D}_{n}\:-\:0\:-\:\theta\:+\:\theta}{{{\sigma}_D}/{\sqrt{n}}} &amp;lt; -1.96 \; | \; \mu_D =\theta]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And then move &lt;span class=&#34;math inline&#34;&gt;\(+\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}\)&lt;/span&gt; to the other sides of the inequalities (be careful with the signs!):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbb{P}[1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}&amp;lt; \frac{\overline{D}_{n}\:-\:\theta}{{{\sigma}_D}/{\sqrt{n}}} &amp;lt; -1.96 -\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}} \; | \; \mu_D =\theta]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[power(\theta) = \mathbb{P}[1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}&amp;lt; Z &amp;lt; -1.96 -\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Which is the same as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[power(\theta) = \mathbb{P}[Z &amp;lt; -1.96 -\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}] + \mathbb{P}[Z &amp;gt; 1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbb{P}[Z &amp;gt; 1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;also can be written&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[1 \: - \: \mathbb{P}[Z &amp;lt; 1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So now we can write our power function for this test:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[power(\theta) = \mathbb{P}[Z &amp;lt; -1.96 +\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}] + 1 \: - \: \mathbb{P}[Z &amp;lt; 1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The notation &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}[Z &amp;lt; z]\)&lt;/span&gt; is usually written &lt;span class=&#34;math inline&#34;&gt;\(N(z)\)&lt;/span&gt; and denotes the &lt;em&gt;area&lt;/em&gt; under the cumulative distribution function up til the value of &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This became much more technical than what I imagined when I started out with this post. I might have to push my amateur adventures in 3D visualisation to the next post! Anyway, now we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[power(\theta) = N(-1.96 +\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}) + 1 - N(1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we assume &lt;span class=&#34;math inline&#34;&gt;\(\sigma_D=1\)&lt;/span&gt; and some sample size this will give us a single value for each &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; we put in. We can easily calculate this in R and plot the power against possible values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; – this is what is known as a power curve. If we instead are interested in seeing how sample size affect power, we hold &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; fixed and vary &lt;em&gt;n&lt;/em&gt;. Below I show the code for for how to calculate and draw the power curve via this function in R for a specified sample size of &lt;span class=&#34;math inline&#34;&gt;\(n=25\)&lt;/span&gt;. In the interactive graph below though, you can adjust the sample size yourself and see how that affects power.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;power_data &amp;lt;- data.frame(theta = seq(from = -1.2, to = 1.2, length.out = 100)) %&amp;gt;% # Create a data frame with a variabl of various theta
  mutate(power = pnorm(-1.96-theta*sqrt(25)) + (1-pnorm(1.96-theta*sqrt(25)))) # Add a variable that for each theta gives us power, according to the power function above


ggplot(power_data, aes(x= theta, y=power)) + # Plot power over theta
  geom_line(size=1) +
  geom_hline(yintercept = 0.05, linetype =&amp;quot;dashed&amp;quot;, color = &amp;quot;darkblue&amp;quot;) +
  geom_vline(xintercept = 0) +
  xlab(expression(paste(theta))) +
  ylab(&amp;quot;Power&amp;quot;) +
  annotate(geom = &amp;quot;text&amp;quot;, x = 0.3, y = 0.08,
           label = &amp;quot;alpha&amp;quot;,
           parse = T,
           color = &amp;quot;darkblue&amp;quot;,
           size = 5) +
  theme_classic()
&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:480px;&#34; class=&#34;widgetframe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;url&#34;:&#34;/post/primary-concepts/index_files/figure-html//widgets/widget_powercurve.html&#34;,&#34;options&#34;:{&#34;xdomain&#34;:&#34;*&#34;,&#34;allowfullscreen&#34;:false,&#34;lazyload&#34;:false}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;As the true effect becomes bigger in absolute values, power goes to 100%. Optimally one would want a power function that wasn’t curved but instead always rejected &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; whenever it was false, how small the effect may be. I.e. we would want a power function that fulfills:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:1&#34;&gt;\[\begin{equation} 
  power =\mathbb{P}[\text{reject} \: H_0 \: | \: \text{any} \: \theta \neq 0] = 1
  \tag{1}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;and&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:2&#34;&gt;\[\begin{equation} 
  power =\mathbb{P}[\text{reject} \: H_0 \: | \: \theta = 0] = 0
  \tag{2}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, tests that fufill these are in practice difficult (if not impossible) to find and we must compromise. For example, equation &lt;a href=&#34;#eq:2&#34;&gt;(2)&lt;/a&gt; tells us that we do not ever want to reject the null hypothesis if it is in fact true. The probability of rejecting the null when the null is true is what we usually refer to as our α-level which we most commonly set to &lt;span class=&#34;math inline&#34;&gt;\(0.05\)&lt;/span&gt; in psychology, i.e. it is the probbility of a Type I error (falsely rejecting the null hypothesis). This compromise can be seen in the power curve above. Additionally, the smaller θ gets, the more difficult the effect is to detect, because the areas under the distribution function shrink.&lt;/p&gt;
&lt;p&gt;When we now know what a power curve is and how it can be derived analytically for a given test, it might be worth mentioning how one can approximate power numerically with simulations. I am not going to use the paired example as I did above, because that requires us to assume some correlation between the measurements. I am going to show the easiest example of simulating power for a t-test between two independent groups, i.e. we assume independence between observations.&lt;/p&gt;
&lt;p&gt;The basic concept behind it is fairly simple, we draw one sample from a normal distributions with a mean of 0 and we draw one sample from a normal distribution with a mean of θ. We then run a t-test to compare the mean differences between the samples to see obtain a p-value. We do this a couple of times and save the p-values for every test. Then we divide the number of significant p-values (p-values &amp;lt; 0.05 in this case) on the total number of tests run and the ratio that we obtain is the power. That is, the power is the number of significant tests, divided by the total number of simulations. The more simulations run, the closer our numerical solution will be to the analytical. Look at the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
power_sim &amp;lt;- function (nsim, theta, n1, n2) {

  sim_mat &amp;lt;- cbind(matrix(rnorm(n1*nsim, mean = theta, sd=1), 
                   nrow = nsim,
                   ncol = n1),
                   matrix(rnorm(n2*nsim, mean = 0, sd=1), 
                   nrow = nsim,
                   ncol = n2))
  
  p_val &amp;lt;- apply(X = sim_mat, MARGIN =  1, 
                        function(x) t.test(x = x[1:n1], y = x[(n1+1):(n1+n2)],
                                           alternative = &amp;quot;two.sided&amp;quot;,
                                           paired = FALSE)[[&amp;quot;p.value&amp;quot;]]
                 )
  
  power &amp;lt;- sum(p_val &amp;lt; 0.05)/length(p_val)
  
  return(power)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Say that we want to see what the power curve would look like over different values of θ, as we did above. We then need to run the simulations for every specified value of θ. This is most easily done in a loop, but you can think of it as if we are running the function above for every θ that are of interest, for example like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;power_sim(nsim = 50, theta = 0.5 , n1 = 25, n2 = 25)

power_sim(nsim = 50, theta = 1.0 , n1 = 25, n2 = 25)

power_sim(nsim = 50, theta = 1.5 , n1 = 25, n2 = 25)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Doing it in a loop however requires a lot less code, so we specify a θ vector, with every value that we are interested in and iteratively run the power simulation function we created for each of those values. The code below does this and then plot the obtained power values over θ for 50 simulations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
theta &amp;lt;- seq(from = -1.2, to = 1.2, length.out = 50) # Here we specify 50 different values of theta

power &amp;lt;- 0 # We need to define an object outside of the loop to be able to iteratively save the power calculations

for (i in 1:length(theta)) { # This is the loop, which runs the code below and saves the power iteratively
  power[i] &amp;lt;- power_sim(nsim = 50, theta[i], n1 = 25, n2 = 25)
}


power_data &amp;lt;- as.data.frame(cbind(power, theta)) # Putting the theta vector and power vector into a data frame 


ggplot(power_data, aes(x= theta, y=power)) +  # Plotting power over theta, just as we did analytically
  geom_line(size=1) +
  geom_hline(yintercept = 0.05, linetype =&amp;quot;dashed&amp;quot;, color = &amp;quot;darkblue&amp;quot;) +
  geom_vline(xintercept = 0) +
  xlab(expression(paste(theta))) +
  ylab(&amp;quot;Power&amp;quot;) +
  annotate(geom = &amp;quot;text&amp;quot;, x = 0.3, y = 0.08, 
           label = &amp;quot;alpha&amp;quot;, 
           parse = T, 
           color = &amp;quot;darkblue&amp;quot;, 
           size = 5) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/primary-concepts/index_files/figure-html/unnamed-chunk-2-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As can be seen, the general pattern is similar to the analytical solution. But since we only ran 50 simulations for every parameter combination we won’t get very precise estimations. So let’s amp it up to 10,000 simulations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
theta &amp;lt;- seq(from = -1.2, to = 1.2, length.out = 50) 

power &amp;lt;- 0 

for (i in 1:length(theta)) { 
  power[i] &amp;lt;- power_sim(nsim = 10000, theta[i], n1 = 25, n2 = 25)
}


power_data &amp;lt;- as.data.frame(cbind(power, theta)) 


ggplot(power_data, aes(x= theta, y=power)) +  # Plotting power over theta, just as we did analytically
  geom_line(size=1) +
  geom_hline(yintercept = 0.05, linetype =&amp;quot;dashed&amp;quot;, color = &amp;quot;darkblue&amp;quot;) +
  geom_vline(xintercept = 0) +
  xlab(expression(paste(theta))) +
  ylab(&amp;quot;Power&amp;quot;) +
  annotate(geom = &amp;quot;text&amp;quot;, x = 0.3, y = 0.08, 
           label = &amp;quot;alpha&amp;quot;, 
           parse = T, 
           color = &amp;quot;darkblue&amp;quot;, 
           size = 5) +
  theme_classic()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/primary-concepts/index_files/figure-html/unnamed-chunk-4-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, this looks more like it! If you compare the analytical power curve with this one you may notice that the first one is more steep - that steepness relects the power advantage of doing paired difference tests instead of independent tests, as we did for the simulation.&lt;/p&gt;
&lt;p&gt;Naturally, when our designs get more complex it will be more difficult to calculate power either analytically or with simulations. But I hope that this post at least showed that you don’t have to be a mathematician to start out with your a priori power calcs!&lt;/p&gt;
&lt;p&gt;Next up: amateur adventures in 3D visualisation of multivariable/variate power curves!&lt;/p&gt;
&lt;!-- P.S. If you want to know how tripling the sample size would effect the curve - have a look below: --&gt;
</description>
    </item>
    
    <item>
      <title>Amateur adventures in 3D visualisation for multivariate power curves</title>
      <link>/post/amateur-adventures-in-3d-visualisation-for-power-curves/</link>
      <pubDate>Fri, 03 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/amateur-adventures-in-3d-visualisation-for-power-curves/</guid>
      <description>



&lt;p&gt;When I first started modelling power, less than a year ago, I had very limited experience with data visualisation. In fact, I was completely new to R and had mainly (but limitedly) worked in Stata and SAS - where my goal was to get a hang of the syntax rather than to understand data formats (even though that might have been more important to be honest). However, I decided to switch to R because of the community surrounding the language, the open source and free nature of it and its potential for being one of the handiest tools for transparent research (i.e. its the best and fun and cool).&lt;/p&gt;
&lt;p&gt;Anyways, I was trying to model power for multivariate hypothesis testing. In the simplest of cases such test only includes two groups and two dependent variables - so this was what I set out to try to model and visualise. The power function for a hypothesis test is defined as the following:&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>fMRI - fraudulent or functional for real?</title>
      <link>/post/fmri-funky-or-fraudulent/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/fmri-funky-or-fraudulent/</guid>
      <description>


&lt;p&gt;We have all seen the colourful dashing pictures of the brain at work. For example the one below from [this and this study]. The most widley used technique to produce these pictures are functional magnetic resonance imaging (fMRI). But what does the coloured pixels actually mean and can they really tell us anything about the cognitive processes going on? Do we give dashes and blobs of colours unreasonably high inferential value just because they are painted on a picture of a brain?&lt;/p&gt;
&lt;p&gt;In this blog post I will give a brief background on how these blobs come about and the inferential caveats that might arise. As an example I will use&lt;/p&gt;
&lt;p&gt;Despite its title, this blog post is not really to conclude one or the other, but rather introduce and line up the caveats of drawing inferences from fMRI data - both for my own sake and anyone interested in this stuff.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Curious Case of the Winner&#39;s Curse</title>
      <link>/post/the-curious-case-of-the-winner-s-curse/</link>
      <pubDate>Tue, 17 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/the-curious-case-of-the-winner-s-curse/</guid>
      <description>&lt;p&gt;Key concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Power = the probability of finding an effect if the effect is true&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Have you ever been at a psychology stats lecture listening to your teacher rambling about &amp;ldquo;power&amp;rdquo;, talking about its importance for your experiment to yield a significant result but without really explaining the &amp;ldquo;why&amp;rdquo; of it all? Well, so have I. It is usually easy enough to grasp that low power = bad, high power = good - it&amp;rsquo;s name kind of gives it away. And given that significance is often treated as the holy grail in psychology most students would want a high probability to obtain it. The advice is often to calculate a sample size based on some previous (or just predicted) effect. However, for a two group experiment with a between group design and expected medium effect (which arguably could be seen as optimistic) we would need 64 participants if we want a probability of 0.8 of detecting the effect.&lt;/p&gt;
&lt;p&gt;For many undergraduate students (and researchers in genereal) this is just not plausible. Yet, a lot of studies and dissertations projects obtain significance anyway. In my undergrad stats course I was told that this is evidence for the great magnitude of the effect, i.e. because the power of a test mainly is influenced by the number of participants and the size of the effect, if one is low in a significant study the other must be high - right? In fact this has been the predominant view by some scholars. Recently I stumbled upon 
&lt;a href=&#34;/pdf/Wuensch.pdf&#34;&gt;this document&lt;/a&gt;, a handout by Karl Wuensch, a psychology professor at East Carolina University from his statistics courses in which he states (note that this is dated 2016):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;hellip;If you were to find a significant effect of the drug with only 25 participants, that would speak to
the large effect of the drug. In this case you should not be hesitant to seek publication of your
research, but you should be somewhat worried about having it reviewed by “ignorant experts.” Such
bozos (and they are to be found everywhere) will argue that your significant results cannot be trusted
because your analysis had little power. It is useless to argue with them, as they are totally lacking in
understanding of the logic of hypothesis testing. If the editor cannot be convinced that the reviewer is
a moron, just resubmit to a different journal and hope to avoid ignorant expert reviewers there.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yes, it is true that significance in a small sample would speak to the large effect found, but what Wuensch seems to miss is that it does speak to the large effect &lt;em&gt;in the sample&lt;/em&gt;. This does not necessarily (and most probably will not) generalise to the population of interest. Perhaps one is not interested in generalising findings and solely in describing one&amp;rsquo;s sample - however, I think that most would agree that this is not what the majortiy of psychologists want.&lt;/p&gt;
&lt;p&gt;But why does not the effect we have found in the sample generalise to the population just because we have low power? Well, intuitively, which score on an IMDB film rating would you rather trust, an average of 9/10 given by 5 people or an average of 6/10 given by 100 people? My guess is that most would say the latter - because we put trust in numbers. In science the above example translates to a phenomenon called the winner&amp;rsquo;s curse. That is the tendency to over estimate the effect size in under powered studies, due to the fact that only the studies that happen to draw a &lt;em&gt;sample&lt;/em&gt; with a large effect will obtain a significant result. Together with publication biases (i.e. significant studies to a higher extent get published) this can yield a serious inflation of what is thought to be population effect sizes.&lt;/p&gt;
&lt;p&gt;To demonstrate this it is useful to look at how such overestimation will affect a field &amp;ldquo;in the long run&amp;rdquo;. If we for example look at the inherently low powered field of single case neuropsychology. That is when you compare a single person (often a patient with some brain lesion) against a normative control group, on some normally distributed function - let&amp;rsquo;s say reaction time. What we want to do is to say whether or not this person has a deficit on this function. Naturally, the probability of detecting such a deficit depends on where in the distribution this patient was located prior to the lesion, i.e. the premorbid ability. Look at the expertly drawn picture below for an example:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;single_case.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Assume a deficit on a this reaction time of ~2 &amp;amp;sigma caused by some lesion – this means that in this lesioned population, a deficit will only be found 50 % of the time (if we operationalise a detection by our patient being 2 &amp;amp;sigma below the population mean), as can be seen in the above illustration. That is, the power of detecting this deficit will &lt;em&gt;never&lt;/em&gt; exceed 50 %. A deficit of 2 σ can never cross the critical value if a patient had a premorbid ability above the mean (σ is here the standard deviation).
The problem of premorbid ability generalises to group lesion studies as well, but more in the sense that it increases variability that can cloud inferences. This calls for the need of increasing sample sizes which often is cumbersome and, in some cases, impossible in the lesioned populations. So, what would this lead to in the long run? By running simulations of studies (in hundreds of thousands) one can approximate how such bias would affect the overall estimation given specific parameters, like the size of the deficit and the number of people in the control sample. I will include my code for such simulation in this specific field below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;win_sim &amp;lt;- function (nsim, deficit, ncon) {
# Matrix where each row represents observations
# on a variable from controls plus a patient
  ranmat &amp;lt;- matrix(rnorm((ncon + 1) * nsim), 
                   nrow = nsim,
                   ncol = ncon + 1)
# Induce a deficit on the first obs of each row
# and compare to the other obs on that row.
# Save the median of all found deficits.
  med_def &amp;lt;- median(
    apply(cl, ranmat, 1, 
       function(x) {if (pt(
              ((x[1] - def) - mean(x[2:ncon+1]))
              /(sd(x[2:ncon+1])*sqrt(length(x)
                                /(length(x) - 1))),
             df =  length(x) - 2)&amp;lt;0.05) {
             (x[1] - def)
                   } else { NA } }), 
              na.rm = TRUE)
# Return the average overestimation
  return(- def -med_def) 
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This simulation returns the median overestimation of as many studies as you choose. If you run it for several different parameter combinations (number of controls and size of deficit). In the graph below I have taken the median of 1000 000 simulated single case studies over functional deficits ranging from 1 to 4 standard deviations for 4 different sizes of control groups.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;
&lt;img src=&#34;/post/curious-case-of/index_files/figure-html/standard-plot-1.svg&#34; alt=&#34;Median overestimation with Crawford and Howell (1998) method, 10^6^ simulations for every parameter combination.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Figure 1: Median overestimation with Crawford and Howell (1998) method, 10^6^ simulations for every parameter combination.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What becomes evident here is that even with a deficit of 2 σ where we would expect a probability of 50 %, we would get a &amp;ldquo;general&amp;rdquo; overestimation of this deficit of about 0.6 σ (which is quite alot). This is due to the fact that we cannot detect the people that have a premobrbid ability above the population mean, if not our control sample &lt;em&gt;by chance&lt;/em&gt; happens to be drawn from the &amp;ldquo;right side&amp;rdquo; side of the distribution (that is we draw a control sample with high reaction time abilities).&lt;/p&gt;
&lt;p&gt;This also explains the curious case when we have a small control sample and a small effect size - yielding a lower overestimation, even though we technically should have lower power. It is easier for a small sample to have a mean that differs from the population mean (think again about the IMDB score), meaning that they can be both higher and lower in an equally distributed manner. However, since our simulated patient solely is diagnosed with a deficit if they have a reaction time that is lower than the control sample mean, this will bias the result in favour of when the control sample are drawn from the right side of the distribution - making it easier to detect smaller deficits and hence the overestimation is lowered.&lt;/p&gt;
&lt;p&gt;What else is evident is that the difference in overestimation between a control sample size of 15 and that of 70 is marginal. Meaning that single case researchers would waste their resources collecting data from larger control sample sizes, given that they want to minimise overestimation.&lt;/p&gt;
&lt;p&gt;This is just one problem that low power yields, and in a very specific field. In the next post I will simulate the effects in more general experiments and talk about other power issues, such as how the positive predictive value is affected. However, I hope that by showing you this, it is clear that low power is not only bad because it will become harder for you to obtain significance in your undergrad thesis. It is bad because, in the long run, it can invalidate our field.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Single Case Neuropsychology: validity and reliability in a power-wise pinioned field (work in progress)</title>
      <link>/publication/neuropsychdiss/</link>
      <pubDate>Thu, 30 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/publication/neuropsychdiss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MANOVA and its contingencies: on the relation between power and correlation - Can the utilisation of multivariate analyses alleviate the power crisis in neuropsychology?</title>
      <link>/publication/statthesis/</link>
      <pubDate>Sun, 12 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/publication/statthesis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terms</title>
      <link>/terms/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/terms/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Oscillatory Differences in EEG - An index for social processing? A social affective perspective on the associative two-process theory of the differential outcomes effect</title>
      <link>/publication/cogdiss/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      <guid>/publication/cogdiss/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
