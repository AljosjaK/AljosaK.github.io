<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jonathan Ö. Rittmo || Blog</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Jonathan Ö. Rittmo || Blog</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 28 Jun 2018 00:00:00 +0100</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Jonathan Ö. Rittmo || Blog</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Power calculations in single-case neuropsychology: a practical primer</title>
      <link>/publication/power_primer/</link>
      <pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/publication/power_primer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Vicarious Value Learning: Knowledge Transfer through Affective Processing on a Social Differential Outcomes Task</title>
      <link>/publication/vic_value/</link>
      <pubDate>Mon, 10 Aug 2020 00:00:00 +0000</pubDate>
      <guid>/publication/vic_value/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Perception, Psychosis and Hellblade</title>
      <link>/post/perception-and-psychosis/</link>
      <pubDate>Sat, 11 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/perception-and-psychosis/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;!-- Imagine that you one day wake up, everything appears normal but when you meet your family for breakfast you appear to speak a different language. No matter how hard you try, they cannot understand you. In fact, it is not only the language - you do not seem to share any common ground. When you are asking for the butter knife they hand you a glass of orange juice. The thought is unsettling, and unfortunately the reality for some. It is not difficult to imagine that if your idea of the world, your picture of what is going on, is unshared, that is a very isolating and terrifying experience. --&gt;
&lt;blockquote&gt;
&lt;p&gt;…external perception is an internal dream which proves to be in harmony with external things; and instead of calling hallucination a false external perception, we must call external perception true hallucination.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above quote from 19th century philospher Hippolyte Taine captures the elusive question: what do we mean by ‘reality’? If you recognise the featured image, you have probably come across the game Hellblade: Senuas Sacrifice, where you follow Senua battle the cracking of her reality. The game was critically acclaimed for depicting the subjective experience of psychosis.&lt;/p&gt;
&lt;p&gt;The game makers worked closely with neuroscientists, one of which was &lt;a href=&#34;https://www.neuroscience.cam.ac.uk/directory/profile.php?pcf22&#34;&gt;Paul Fletcher&lt;/a&gt;, a professor at Cambridge who has devoted much of his career to understanding the mechanisms of this disorder. I attended a talk of his a couple of months ago where he mentioned the game which immediately spiked my interest. Due to the current circumstances I had the chance to play through it this weekend - and it was a ride. I know my last post said I was gonna write more about power curves but after playing the game I couldn’t resist a small post on this subject. A version of Fletcher’s talk, in which he conveys the keystone ideas from his 20 years of research is available &lt;a href=&#34;https://www.youtube.com/watch?v=9rE5A1DDAFM&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To get as immersed as possible I played the game in complete darkness, with noise cancelling headphones and the volume high (which I sincerly recommend, since they put a lot of effort in to the audio). I nearly completed it in one sitting which also was prefereable for screening off as much of my world as possible and delve into that of Senua’s psychosis. In this post I am going to go through some of the ideas they based the game on from Fletcher’s research ‒ which really shines through in both script and experience.&lt;/p&gt;
&lt;p&gt;The term ‘psychosis’ is not uncontroversial. If we define it loosely as losing contact with reality, it is easy to see why. Because what makes our healthy perception real? Fletcher makes the case that we are all a bit out of touch with reality. Most people simply hallucinate true events.&lt;/p&gt;
&lt;p&gt;This might be difficult to grasp. But since your brain is encased in a skull and has no direct contact with the outside world it must create a predictive model of it. Hence, Fletcher argues, psychotic illness is just a small variation in normal perceptive processing.&lt;/p&gt;
&lt;p&gt;Why do we have a brain? A fundemental fact of biological life is that the genes that most easily continue to exist tend to do just that. In a complex and noisy world, a machine that can make predictions about it would benefit any organism.&lt;/p&gt;
&lt;p&gt;In a sense then, our brain is like a scientist, trying to draw inferences and make predictions from incomplete data. To do this one utilises expectations. Information from previous experiences that make our predictions more or less likely. If our predictions do not match the external world we must update them (in science and in life) with the given feedback. One can imagine the power of such device just by looking at how science has benefitted society.&lt;/p&gt;
&lt;p&gt;This creates loops of predictions, inputs and prediction errors that integrate to form a stable perception. It is a breakdown in these integration processes that causes the psychotic state, Fletcher argues.&lt;/p&gt;
&lt;p&gt;Perception has two foundations: input and predictions. Take a look at the &lt;a href=&#34;https://www.youtube.com/watch?v=pH9dAbPOR6M&#34;&gt;hollow mask illusion&lt;/a&gt;:&lt;/p&gt;
&lt;!-- ![](hollow_face3.png) ![](hollow_face2.png) ![](hollow_face1.png) --&gt;
&lt;p&gt;&lt;img src=&#34;hollow_face3.png&#34; width=&#34;20%&#34; height=&#34;25%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;hollow_face2.png&#34; width=&#34;20%&#34; height=&#34;25%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;hollow_face1.png&#34; width=&#34;20%&#34; height=&#34;25%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the bottommost image we see a convex surface. The facial features however, make us expect it to be concave. Therefore it appears as such. It is easy to imagine how more abstract ideas could shape perceived input as well. For example, &lt;a href=&#34;https://blogs.scientificamerican.com/observations/how-do-you-know-which-emotion-a-facial-expression-represents/&#34;&gt;facial expressions being interpreted differently depending on contexts&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This mechanism can also be &lt;em&gt;additive&lt;/em&gt;. Take the Kanizsa triangle, for example, where it is almost impossible not to see a white triangle laying over the other shapes even though it is not there:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pngwave.png&#34; width=&#34;100%&#34; height=&#34;25%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So, expectations &lt;em&gt;create&lt;/em&gt; perceptions. Fletcher describes a system that tries to maintain stability between predictions and input. To best align your internal world with the external. This happens at multiple levels ‒ from abstract (expressions) to concrete (Kanizsa triangle). Below is my interpretation of this system, W = weight.&lt;/p&gt;
&lt;!-- *Note*: this is *my* interpretation of Fletcher&#39;s model. W = weight. --&gt;
&lt;p&gt;&lt;img src=&#34;boxandarrow.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can see that a balance between input and predictions must be maintained. If one gets too weighted it affects perception. For example if the input is cut off, predictions get an overbalance and hence your mind takes control of your perception and hallucinations might be induced - which often happen during e.g. &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/19829208&#34;&gt;sensory deprivation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Fletcher also mention that traumatic experiences, such as child abuse, could cause abstract predictions being unreasonably weighted. E.g. predicting low self worth could in a downstream manner affect how you see and interpret aspects of the world. Perhaps it would seem more ill-willed. Similarily such prediction might cause additive percepts of, for example, derogatory voices. This would correspond to the two cornerstones of pyschosis, delusion and hallucination respectively.&lt;/p&gt;
&lt;p&gt;Psychotic traits is, by this model, hence formed by too strong predictions. But strong predictions can also aid your perception. Imagine looking at a painting in your home during night. Since you know it, it’s easier to see the contours, right? If you don’t believe me, try to discern the image below:&lt;/p&gt;
&lt;!-- It also means that hallucination prone individuals would be be more sensitive to detect stimuli to which they have already been exposed. Because their perception is to a higher degree weighted by prediction. Try to discern what the picture below depicts without scrolling. --&gt;
&lt;!-- Like any explanatory model worth its name this gives us a few testable hypotheses. One is that hallucination prone people would be more sensitive to detect stimuli one has already been exposed to. Because your perception is more heavily based on prediction and hence previous experience. Try to discern what the picture below depicts without scrolling. --&gt;
&lt;p&gt;&lt;img src=&#34;tanntannbw.png&#34; width=&#34;100%&#34; height=&#34;25%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not very easy, right? I, however, cannot not see myself as a kid:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;tanntann.png&#34; width=&#34;100%&#34; height=&#34;25%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Given Fletcher’s model, hallucination prone individuals would thus be &lt;em&gt;better&lt;/em&gt; at these types of tasks (given previous exposure, so that they have something to expect). This was also &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/26460044&#34;&gt;found&lt;/a&gt; by him and colleagues.&lt;/p&gt;
&lt;p&gt;Further evidence both for and against the model is reviewed in the &lt;a href=&#34;https://www.youtube.com/watch?v=9rE5A1DDAFM&#34;&gt;talk&lt;/a&gt;. But I do not want to spoil all the fun and advise you to take a look at it.&lt;/p&gt;
&lt;p&gt;I should also make it clear that Fletcher does not claim that this model paint a complete picture. I for one am a bit sceptical about the hierarchical structure of the integration processes. However, if you are interested in this stuff (who wouldn’t be interested in what reality is, eh?) Fletcher makes a compelling case. Playing through Hellblade after you’ve watched the talk makes it even more compelling as the ideas are communicated by experiencing them ‒ which is both thought provoking and generates an understanding and compassion for the people suffering from this. (You could also use drugs, but I’d give Hellblade a go first).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Primary Concepts of Power Curves</title>
      <link>/post/primary-concepts-of-power-curves/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/primary-concepts-of-power-curves/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/pymjs/pym.v1.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/widgetframe-binding/widgetframe.js&#34;&gt;&lt;/script&gt;


&lt;!-- This post did not at all become what I thought it would when I started writing it. I was going to write about visualisation of multivariate power curves, but halfway through of explaining what a power curve is I realised that it probably would be better to split the post into two. So here comes the first half! --&gt;
&lt;p&gt;As the image above (probably abused by me) clearly states: “the abuse of power comes as no surprise”. Whether intentional or not, ignorance of a priori power calculations have been prominent in psychology and related fields.&lt;/p&gt;
&lt;p&gt;But this is not surprising, because it is difficult to grasp what a power calculation actually is. In this post I will walk through some of the basic calculations behind deriving the power function for a paired samples z-test analytically (that is obtaining an exact mathematical function) and how you can calculate power for an independent two sample t-test, by simulations in R. Hopefully this will show that the basic concepts of power calculations and power curves actually are quite easy! So here goes:&lt;/p&gt;
&lt;p&gt;The power function for a hypothesis test is defined as the following:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[power(\theta) =\mathbb{P}[\text{reject} \: H_0 \: \text{for a given} \: \theta]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is a given effect size, &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}\)&lt;/span&gt; probability and &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; the null hypothesis. In more general terms it is defined (the vertical bar is read “given”):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[power =\mathbb{P}[\text{reject} \: H_0\:  | \: H_1 \: \text{is true}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Say that we want to test whether a medication has had some effect on a sample (we measure a continuous dependent variable on the same individuals before and after treatment) and we assume that we know the population standard deviation. If &lt;span class=&#34;math inline&#34;&gt;\(\overline{D}\)&lt;/span&gt; is the average difference between the observations, for each individual then &lt;span class=&#34;math inline&#34;&gt;\(\overline{D} \sim N(\mu, \sigma^2)\)&lt;/span&gt;, meaning that &lt;span class=&#34;math inline&#34;&gt;\(\overline{D}\)&lt;/span&gt; follows the normal distribution with some population mean (&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;) and variance (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;). Standardising &lt;span class=&#34;math inline&#34;&gt;\(\overline{D}\)&lt;/span&gt; by centering it around 0 (since we are talking about a difference it is already centered around 0 though) and dividing by the standard error we would have an entity that follows the standard normal distribution:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\overline{D}-0}{{{\sigma}_D}/{\sqrt{n}}}\sim Z\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(\overline{D}\)&lt;/span&gt; is the average of the differences between the two measurement time points and &lt;span class=&#34;math inline&#34;&gt;\({\sigma}_D\)&lt;/span&gt; the population standard deviation and hence &lt;span class=&#34;math inline&#34;&gt;\({{\sigma}_D}/{\sqrt{n}}\)&lt;/span&gt; the standard error and thus, if we input real values we would get a z-value. The null and alternative hypotheses for this test are &lt;span class=&#34;math inline&#34;&gt;\(H_0:\: \mu_D \: = \: 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(H_1:\: \mu_D \: \neq \: 0\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\mu_D\)&lt;/span&gt; is estimated by &lt;span class=&#34;math inline&#34;&gt;\(\overline{D}_n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If we set &lt;span class=&#34;math inline&#34;&gt;\(\alpha\:=\:0.05\)&lt;/span&gt; (i.e. &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}\)&lt;/span&gt;[reject &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; | &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; is true]&lt;span class=&#34;math inline&#34;&gt;\(\:=\:0.05\)&lt;/span&gt;) our critical z-value would be &lt;span class=&#34;math inline&#34;&gt;\(z_{(0.05/2) }=\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\pm\)&lt;/span&gt; 1.96. If you are unfamiliar with critical values, have a look at the density plot below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/primary-concepts/index_files/figure-html/zdist-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;All observed z-values that fall above the upper or below the lower critical cutoff values are considered significant at the &lt;span class=&#34;math inline&#34;&gt;\(\alpha \: = \: 0.05\)&lt;/span&gt; level. Now suppose that &lt;span class=&#34;math inline&#34;&gt;\(\mu_D \: \neq \: 0\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\mu_D =\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Then we get the power by calculating:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbb{P}[1.96 &amp;lt; Z &amp;lt; -1.96 \; | \; \mu_D =\theta]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(\frac{\overline{D}_n-0}{{{\sigma}_D}/{\sqrt{n}}}\sim Z\)&lt;/span&gt; we’ll write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbb{P}[1.96 &amp;lt; \frac{\overline{D}_{n}-0}{{{\sigma}_D}/{\sqrt{n}}} &amp;lt; -1.96 \; | \; \mu_D =\theta]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We’ll subtract and add &lt;span class=&#34;math inline&#34;&gt;\(\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}\)&lt;/span&gt;, i.e. a neutral operation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbb{P}[1.96 &amp;lt; \frac{\overline{D}_{n}\:-\:0\:-\:\theta\:+\:\theta}{{{\sigma}_D}/{\sqrt{n}}} &amp;lt; -1.96 \; | \; \mu_D =\theta]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And then move &lt;span class=&#34;math inline&#34;&gt;\(+\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}\)&lt;/span&gt; to the other sides of the inequalities (be careful with the signs!):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbb{P}[1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}&amp;lt; \frac{\overline{D}_{n}\:-\:\theta}{{{\sigma}_D}/{\sqrt{n}}} &amp;lt; -1.96 -\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}} \; | \; \mu_D =\theta]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[power(\theta) = \mathbb{P}[1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}&amp;lt; Z &amp;lt; -1.96 -\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Which is the same as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[power(\theta) = \mathbb{P}[Z &amp;lt; -1.96 -\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}] + \mathbb{P}[Z &amp;gt; 1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbb{P}[Z &amp;gt; 1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;also can be written&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[1 \: - \: \mathbb{P}[Z &amp;lt; 1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So now we can write our power function for this test:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[power(\theta) = \mathbb{P}[Z &amp;lt; -1.96 +\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}] + 1 \: - \: \mathbb{P}[Z &amp;lt; 1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The notation &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{P}[Z &amp;lt; z]\)&lt;/span&gt; is usually written &lt;span class=&#34;math inline&#34;&gt;\(N(z)\)&lt;/span&gt; and denotes the &lt;em&gt;area&lt;/em&gt; under the cumulative distribution function up til the value of &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This became much more technical than what I imagined when I started out with this post. I might have to push my amateur adventures in 3D visualisation to the next post! Anyway, now we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[power(\theta) = N(-1.96 +\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}) + 1 - N(1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we assume &lt;span class=&#34;math inline&#34;&gt;\(\sigma_D=1\)&lt;/span&gt; and some sample size this will give us a single value for each &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; we put in. We can easily calculate this in R and plot the power against possible values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; – this is what is known as a power curve. If we instead are interested in seeing how sample size affect power, we hold &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; fixed and vary &lt;em&gt;n&lt;/em&gt;. Below I show the code for for how to calculate and draw the power curve via this function in R for a specified sample size of &lt;span class=&#34;math inline&#34;&gt;\(n=25\)&lt;/span&gt;. In the interactive graph below though, you can adjust the sample size yourself and see how that affects power.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;power_data &amp;lt;- data.frame(theta = seq(from = -1.2, to = 1.2, length.out = 100)) %&amp;gt;% # Create a data frame with a variabl of various theta
  mutate(power = pnorm(-1.96-theta*sqrt(25)) + (1-pnorm(1.96-theta*sqrt(25)))) # Add a variable that for each theta gives us power, according to the power function above


ggplot(power_data, aes(x= theta, y=power)) + # Plot power over theta
  geom_line(size=1) +
  geom_hline(yintercept = 0.05, linetype =&amp;quot;dashed&amp;quot;, color = &amp;quot;darkblue&amp;quot;) +
  geom_vline(xintercept = 0) +
  xlab(expression(paste(theta))) +
  ylab(&amp;quot;Power&amp;quot;) +
  annotate(geom = &amp;quot;text&amp;quot;, x = 0.3, y = 0.08,
           label = &amp;quot;alpha&amp;quot;,
           parse = T,
           color = &amp;quot;darkblue&amp;quot;,
           size = 5) +
  theme_classic()
&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:480px;&#34; class=&#34;widgetframe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;url&#34;:&#34;/post/primary-concepts/index_files/figure-html//widgets/widget_powercurve.html&#34;,&#34;options&#34;:{&#34;xdomain&#34;:&#34;*&#34;,&#34;allowfullscreen&#34;:false,&#34;lazyload&#34;:false}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;As the true effect becomes bigger in absolute values, power goes to 100%. Optimally one would want a power function that wasn’t curved but instead always rejected &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; whenever it was false, how small the effect may be. I.e. we would want a power function that fulfills:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:1&#34;&gt;\[\begin{equation} 
  power =\mathbb{P}[\text{reject} \: H_0 \: | \: \text{any} \: \theta \neq 0] = 1
  \tag{1}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;and&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:2&#34;&gt;\[\begin{equation} 
  power =\mathbb{P}[\text{reject} \: H_0 \: | \: \theta = 0] = 0
  \tag{2}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, tests that fulfill these are in practice difficult (if not impossible) to find and we must compromise. For example, equation &lt;a href=&#34;#eq:2&#34;&gt;(2)&lt;/a&gt; tells us that we do not ever want to reject the null hypothesis if it is in fact true. The probability of rejecting the null when the null is true is what we usually refer to as our α-level which we most commonly set to &lt;span class=&#34;math inline&#34;&gt;\(0.05\)&lt;/span&gt; in psychology, i.e. it is the probability of a Type I error (falsely rejecting the null hypothesis). This compromise can be seen in the power curve above. Additionally, the smaller θ gets, the more difficult the effect is to detect, because the areas under the distribution function shrink.&lt;/p&gt;
&lt;p&gt;When we now know what a power curve is and how it can be derived analytically for a given test, it might be worth mentioning how one can approximate power numerically with simulations. I am not going to use the paired example as I did above, because that requires us to assume some correlation between the measurements. I am going to show the easiest example of simulating power for a t-test between two independent groups, i.e. we assume independence between observations.&lt;/p&gt;
&lt;p&gt;The basic concept behind it is fairly simple, we draw one sample from a normal distributions with a mean of 0 and we draw one sample from a normal distribution with a mean of θ. We then run a t-test to compare the mean differences between the samples to see obtain a p-value. We do this a couple of times and save the p-values for every test. Then we divide the number of significant p-values (p-values &amp;lt; 0.05 in this case) on the total number of tests run and the ratio that we obtain is the power. That is, the power is the number of significant tests, divided by the total number of simulations. The more simulations run, the closer our numerical solution will be to the analytic. Look at the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
power_sim &amp;lt;- function (nsim, theta, n1, n2) {

  sim_mat &amp;lt;- cbind(matrix(rnorm(n1*nsim, mean = theta, sd=1), 
                   nrow = nsim,
                   ncol = n1),
                   matrix(rnorm(n2*nsim, mean = 0, sd=1), 
                   nrow = nsim,
                   ncol = n2))
  
  p_val &amp;lt;- apply(X = sim_mat, MARGIN =  1, 
                        function(x) t.test(x = x[1:n1], y = x[(n1+1):(n1+n2)],
                                           alternative = &amp;quot;two.sided&amp;quot;,
                                           paired = FALSE)[[&amp;quot;p.value&amp;quot;]]
                 )
  
  power &amp;lt;- sum(p_val &amp;lt; 0.05)/length(p_val)
  
  return(power)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Say that we want to see what the power curve would look like over different values of θ, as we did above. We then need to run the simulations for every specified value of θ. This is most easily done in a loop, but you can think of it as if we are running the function above for every θ that are of interest, for example like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;power_sim(nsim = 50, theta = 0.5 , n1 = 25, n2 = 25)

power_sim(nsim = 50, theta = 1.0 , n1 = 25, n2 = 25)

power_sim(nsim = 50, theta = 1.5 , n1 = 25, n2 = 25)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Doing it in a loop however requires a lot less code, so we specify a θ vector, with every value that we are interested in and iteratively run the power simulation function we created for each of those values. The code below does this and then plot the obtained power values over θ for 50 simulations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
theta &amp;lt;- seq(from = -1.2, to = 1.2, length.out = 50) # Here we specify 50 different values of theta

power &amp;lt;- 0 # We need to define an object outside of the loop to be able to iteratively save the power calculations

for (i in 1:length(theta)) { # This is the loop, which runs the code below and saves the power iteratively
  power[i] &amp;lt;- power_sim(nsim = 50, theta[i], n1 = 25, n2 = 25)
}


power_data &amp;lt;- as.data.frame(cbind(power, theta)) # Putting the theta vector and power vector into a data frame 


ggplot(power_data, aes(x= theta, y=power)) +  # Plotting power over theta, just as we did analytically
  geom_line(size=1) +
  geom_hline(yintercept = 0.05, linetype =&amp;quot;dashed&amp;quot;, color = &amp;quot;darkblue&amp;quot;) +
  geom_vline(xintercept = 0) +
  xlab(expression(paste(theta))) +
  ylab(&amp;quot;Power&amp;quot;) +
  annotate(geom = &amp;quot;text&amp;quot;, x = 0.3, y = 0.08, 
           label = &amp;quot;alpha&amp;quot;, 
           parse = T, 
           color = &amp;quot;darkblue&amp;quot;, 
           size = 5) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/primary-concepts/index_files/figure-html/unnamed-chunk-2-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As can be seen, the general pattern is similar to the analytic solution. But since we only ran 50 simulations for every parameter combination we won’t get very precise estimations. So let’s amp it up to 10,000 simulations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
theta &amp;lt;- seq(from = -1.2, to = 1.2, length.out = 50) 

power &amp;lt;- 0 

for (i in 1:length(theta)) { 
  power[i] &amp;lt;- power_sim(nsim = 10000, theta[i], n1 = 25, n2 = 25)
}


power_data &amp;lt;- as.data.frame(cbind(power, theta)) 


ggplot(power_data, aes(x= theta, y=power)) +  # Plotting power over theta, just as we did analytically
  geom_line(size=1) +
  geom_hline(yintercept = 0.05, linetype =&amp;quot;dashed&amp;quot;, color = &amp;quot;darkblue&amp;quot;) +
  geom_vline(xintercept = 0) +
  xlab(expression(paste(theta))) +
  ylab(&amp;quot;Power&amp;quot;) +
  annotate(geom = &amp;quot;text&amp;quot;, x = 0.3, y = 0.08, 
           label = &amp;quot;alpha&amp;quot;, 
           parse = T, 
           color = &amp;quot;darkblue&amp;quot;, 
           size = 5) +
  theme_classic()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/primary-concepts/index_files/figure-html/unnamed-chunk-4-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, this looks more like it! This approach is a useful tool for a priori power calculations when an analytic solution is too difficult (sometimes it’s even impossible) to find. But naturally, when our designs get more complex it will be more difficult to calculate power either analytically or with simulations. But I hope that this post at least showed that you don’t have to be a mathematician to start out with your a priori power calcs!&lt;/p&gt;
&lt;p&gt;Next up: amateur adventures in 3D visualisation of multivariable/variate power curves!&lt;/p&gt;
&lt;!-- P.S. If you want to know how tripling the sample size would effect the curve - have a look below: --&gt;
</description>
    </item>
    
    <item>
      <title>Amateur adventures in 3D visualisation for multivariate power curves</title>
      <link>/post/amateur-adventures-in-3d-visualisation-for-power-curves/</link>
      <pubDate>Fri, 03 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/amateur-adventures-in-3d-visualisation-for-power-curves/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;When I first started modelling power, less than a year ago, I had very limited experience with data visualisation. In fact, I was completely new to R and had mainly (but limitedly) worked in Stata and SAS - where my goal was to get a hang of the syntax rather than to understand data formats (even though that might have been more important to be honest). However, I decided to switch to R because of the community surrounding the language, the open source and free nature of it and its potential for being one of the handiest tools for transparent research (i.e. its the best and fun and cool).&lt;/p&gt;
&lt;p&gt;Anyways, I was trying to model power for multivariate hypothesis testing. In the simplest of cases such test only includes two groups and two dependent variables - so this was what I set out to try to model and visualise. The power function for a hypothesis test is defined as the following:&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>fMRI - fraudulent or functional for real?</title>
      <link>/post/fmri-funky-or-fraudulent/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/fmri-funky-or-fraudulent/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;We have all seen the colourful dashing pictures of the brain at work. For example the one below from [this and this study]. The most widley used technique to produce these pictures are functional magnetic resonance imaging (fMRI). But what does the coloured pixels actually mean and can they really tell us anything about the cognitive processes going on? Do we give dashes and blobs of colours unreasonably high inferential value just because they are painted on a picture of a brain?&lt;/p&gt;
&lt;p&gt;In this blog post I will give a brief background on how these blobs come about and the inferential caveats that might arise. As an example I will use&lt;/p&gt;
&lt;p&gt;Despite its title, this blog post is not really to conclude one or the other, but rather introduce and line up the caveats of drawing inferences from fMRI data - both for my own sake and anyone interested in this stuff.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Curious Case of the Winner&#39;s Curse</title>
      <link>/post/the-curious-case-of-the-winner-s-curse/</link>
      <pubDate>Tue, 17 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/the-curious-case-of-the-winner-s-curse/</guid>
      <description>&lt;p&gt;Key concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Power = the probability of finding an effect if the effect is true&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Have you ever been at a psychology stats lecture listening to your teacher rambling about &amp;ldquo;power&amp;rdquo;, talking about its importance for your experiment to yield a significant result but without really explaining the &amp;ldquo;why&amp;rdquo; of it all? Well, so have I. It is usually easy enough to grasp that low power = bad, high power = good - it&amp;rsquo;s name kind of gives it away. And given that significance is often treated as the holy grail in psychology most students would want a high probability to obtain it. The advice is often to calculate a sample size based on some previous (or just predicted) effect. However, for a two group experiment with a between group design and expected medium effect (which arguably could be seen as optimistic) we would need 64 participants if we want a probability of 0.8 of detecting the effect.&lt;/p&gt;
&lt;p&gt;For many undergraduate students (and researchers in genereal) this is just not plausible. Yet, a lot of studies and dissertations projects obtain significance anyway. In my undergrad stats course I was told that this is evidence for the great magnitude of the effect, i.e. because the power of a test mainly is influenced by the number of participants and the size of the effect, if one is low in a significant study the other must be high - right? In fact this has been the predominant view by some scholars. Recently I stumbled upon 
&lt;a href=&#34;/pdf/Wuensch.pdf&#34;&gt;this document&lt;/a&gt;, a handout by Karl Wuensch, a psychology professor at East Carolina University from his statistics courses in which he states (note that this is dated 2016):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;hellip;If you were to find a significant effect of the drug with only 25 participants, that would speak to
the large effect of the drug. In this case you should not be hesitant to seek publication of your
research, but you should be somewhat worried about having it reviewed by “ignorant experts.” Such
bozos (and they are to be found everywhere) will argue that your significant results cannot be trusted
because your analysis had little power. It is useless to argue with them, as they are totally lacking in
understanding of the logic of hypothesis testing. If the editor cannot be convinced that the reviewer is
a moron, just resubmit to a different journal and hope to avoid ignorant expert reviewers there.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yes, it is true that significance in a small sample would speak to the large effect found, but what Wuensch seems to miss is that it does speak to the large effect &lt;em&gt;in the sample&lt;/em&gt;. This does not necessarily (and most probably will not) generalise to the population of interest. Perhaps one is not interested in generalising findings and solely in describing one&amp;rsquo;s sample - however, I think that most would agree that this is not what the majortiy of psychologists want.&lt;/p&gt;
&lt;p&gt;But why does not the effect we have found in the sample generalise to the population just because we have low power? Well, intuitively, which score on an IMDB film rating would you rather trust, an average of 9/10 given by 5 people or an average of 6/10 given by 100 people? My guess is that most would say the latter - because we put trust in numbers. In science the above example translates to a phenomenon called the winner&amp;rsquo;s curse. That is the tendency to over estimate the effect size in under powered studies, due to the fact that only the studies that happen to draw a &lt;em&gt;sample&lt;/em&gt; with a large effect will obtain a significant result. Together with publication biases (i.e. significant studies to a higher extent get published) this can yield a serious inflation of what is thought to be population effect sizes.&lt;/p&gt;
&lt;p&gt;To demonstrate this it is useful to look at how such overestimation will affect a field &amp;ldquo;in the long run&amp;rdquo;. If we for example look at the inherently low powered field of single case neuropsychology. That is when you compare a single person (often a patient with some brain lesion) against a normative control group, on some normally distributed ability - let&amp;rsquo;s say IQ. What we want to do is to say whether or not this person has a deficit on this function. Naturally, the probability of detecting such a deficit depends on where in the distribution this patient was located prior to the lesion, i.e. the premorbid ability. Look at the expertly drawn picture below for an example:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;single_case.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Assume a deficit on IQ of ~2 &amp;amp;sigma caused by some lesion – this means that in this lesioned population, a deficit will only be found 50 % of the time (if we operationalise a detection by our patient being 2 &amp;amp;sigma below the population mean), as can be seen in the above illustration. That is, the power of detecting this deficit will &lt;em&gt;never&lt;/em&gt; exceed 50 %. A deficit of 2 σ can never cross the critical value if a patient had a premorbid ability above the mean (σ is here the standard deviation).
The problem of premorbid ability generalises to group lesion studies as well, but more in the sense that it increases variability that can cloud inferences. This calls for the need of increasing sample sizes which often is cumbersome and, in some cases, impossible in the lesioned populations. So, what would this lead to in the long run? By running simulations of studies (in hundreds of thousands) one can approximate how such bias would affect the overall estimation given specific parameters, like the size of the deficit and the number of people in the control sample. I will include my code for such simulation in this specific field below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;win_sim &amp;lt;- function (nsim, deficit, ncon) {
# Matrix where each row represents observations
# on a variable from controls plus a patient
  ranmat &amp;lt;- matrix(rnorm((ncon + 1) * nsim), 
                   nrow = nsim,
                   ncol = ncon + 1)
# Induce a deficit on the first obs of each row
# and compare to the other obs on that row.
# Save the median of all found deficits.
  med_def &amp;lt;- median(
    apply(cl, ranmat, 1, 
       function(x) {if (pt(
              ((x[1] - def) - mean(x[2:ncon+1]))
              /(sd(x[2:ncon+1])*sqrt(length(x)
                                /(length(x) - 1))),
             df =  length(x) - 2)&amp;lt;0.05) {
             (x[1] - def)
                   } else { NA } }), 
              na.rm = TRUE)
# Return the average overestimation
  return(- def -med_def) 
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This simulation returns the median overestimation of as many studies as you choose. If you run it for several different parameter combinations (number of controls and size of deficit). In the graph below I have taken the median of 1000 000 simulated single case studies over functional deficits ranging from 1 to 4 standard deviations for 4 different sizes of control groups.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: left&#34;&gt;
&lt;img src=&#34;/post/curious-case-of/index_files/figure-html/standard-plot-1.svg&#34; alt=&#34;Median overestimation with Crawford and Howell (1998) method, 10^6^ simulations for every parameter combination.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Figure 1: Median overestimation with Crawford and Howell (1998) method, 10^6^ simulations for every parameter combination.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What becomes evident here is that even with a deficit of 2 σ where we would expect a probability of 50 %, we would get a &amp;ldquo;general&amp;rdquo; overestimation of this deficit of about 0.6 σ (which is quite alot). This is due to the fact that we cannot detect the people that have a premobrbid ability above the population mean, if not our control sample &lt;em&gt;by chance&lt;/em&gt; happens to be drawn from the &amp;ldquo;right side&amp;rdquo; side of the distribution (that is we draw a control sample with high IQ).&lt;/p&gt;
&lt;p&gt;This also explains the curious case when we have a small control sample and a small effect size - yielding a lower overestimation, even though we technically should have lower power. It is easier for a small sample to have a mean that differs from the population mean (think again about the IMDB score), meaning that they can be both higher and lower in an equally distributed manner. However, since our simulated patient solely is diagnosed with a deficit if they have an IQ lower than the control sample mean, this will bias the result in favour of when the control sample are drawn from the right side of the distribution - making it easier to detect smaller deficits and hence the overestimation is lowered.&lt;/p&gt;
&lt;p&gt;What else is evident is that the difference in overestimation between a control sample size of 15 and that of 70 is marginal. Meaning that single case researchers would waste their resources collecting data from larger control sample sizes, given that they want to minimise overestimation.&lt;/p&gt;
&lt;p&gt;This is just one problem that low power yields, and in a very specific field. In the next post I will simulate the effects in more general experiments and talk about other power issues, such as how the positive predictive value is affected. However, I hope that by showing you this, it is clear that low power is not only bad because it will become harder for you to obtain significance in your undergrad thesis. It is bad because, in the long run, it can invalidate our field.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Single case neuropsychology: Validity of hypothesis testing in a power-wise pinioned field</title>
      <link>/publication/neuropsychdiss/</link>
      <pubDate>Thu, 30 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/publication/neuropsychdiss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MANOVA and its contingencies: on the relation between power and correlation - Can the utilisation of multivariate analyses alleviate the power crisis in neuropsychology?</title>
      <link>/publication/statthesis/</link>
      <pubDate>Sun, 12 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/publication/statthesis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terms</title>
      <link>/terms/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/terms/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Oscillatory Differences in EEG - An index for social processing? A social affective perspective on the associative two-process theory of the differential outcomes effect</title>
      <link>/publication/cogdiss/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      <guid>/publication/cogdiss/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
