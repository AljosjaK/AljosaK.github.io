[{"authors":["admin"],"categories":null,"content":"Brain damaged brain student writing about methodological and statistical issues in cognitive neuroscience, as well as other related topics. If I feel inclined I might write more technical posts and if you\u0026rsquo;re real unlucky there might even slip in some fiction book reviews. I will only post about my mental fatigue when I have the energy.\n","date":1580342400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1580342400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Brain damaged brain student writing about methodological and statistical issues in cognitive neuroscience, as well as other related topics. If I feel inclined I might write more technical posts and if you\u0026rsquo;re real unlucky there might even slip in some fiction book reviews.","tags":null,"title":"Jonathan Ö. Rittmo","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Tutorials on my upcoming R package for single case research will be uploaded shortly.","tags":null,"title":"","type":"docs"},{"authors":[],"categories":["research"],"content":"Key concepts:\n Power = the probability of finding an effect if the effect is true  Have you ever been at a psychology stats lecture listening to your teacher rambling about \u0026ldquo;power\u0026rdquo;, talking about its importance for your experiment to yield a significant result but without really explaining the \u0026ldquo;why\u0026rdquo; of it all? Well, so have I. It is usually easy enough to grasp that low power = bad, high power = good - it\u0026rsquo;s name kind of gives it away. And given that significance is often treated as the holy grail in psychology most students would want a high probability to obtain it. The advice is often to calculate a sample size based on some previous (or just predicted) effect. However, for a two group experiment with a between group design and expected medium effect (which arguably could be seen as optimistic) we would need 64 participants if we want a probability of 0.8 of detecting the effect.\nFor many undergraduate students (and researchers in genereal) this is just not plausible. Yet, a lot of studies and dissertations projects obtain significance anyway. In my undergrad stats course I was told that this is evidence for the great magnitude of the effect, i.e. because the power of a test mainly is influenced by the number of participants and the size of the effect, if one is low in a significant study the other must be high - right? In fact this has been the predominant view by some scholars. Recently I stumbled upon this document, a handout by Karl Wuensch, a psychology professor at East Carolina University from his statistics courses in which he states:\n \u0026hellip;If you were to find a significant effect of the drug with only 25 participants, that would speak to the large effect of the drug. In this case you should not be hesitant to seek publication of your research, but you should be somewhat worried about having it reviewed by “ignorant experts.” Such bozos (and they are to be found everywhere) will argue that your significant results cannot be trusted because your analysis had little power. It is useless to argue with them, as they are totally lacking in understanding of the logic of hypothesis testing. If the editor cannot be convinced that the reviewer is a moron, just resubmit to a different journal and hope to avoid ignorant expert reviewers there.\n Yes, it is true that significance in a small sample would speak to the large effect found, but what Wuensch seems to miss is that it does speak to the large effect in the sample. This does not necessarily (and most probably not) generalise to the population of interest. Perhaps one is not interested in generalising findings and solely in describing one\u0026rsquo;s sample - however, I think that most would agree that this is not what the majortiy of psychologists want.\nBut why does not the effect we have found in the sample generalise to the population just because we have low power? Well, intuitively, which score on an IMDB film rating would you rather trust, an average of 9/10 given by 5 people or an average of 6/10 given by 100 people? My guess is that most would say the latter - because we put trust in numbers. In science the above example translates to a phenomenon called the winner\u0026rsquo;s curse. That is the tendency to over estimate the effect size in under powered studies, due to the fact that only the studies that happen to draw a sample with a large effect will obtain a significant result. Together with publication biases (i.e. significant studies to a higher extent get published) this can yield a serious inflation of what is thought to be population effect sizes.\nTo demonstrate this it is useful to look at how such overestimation will affect a field \u0026ldquo;in the long run\u0026rdquo;. If we for example look at the inherently low powered field of single case neuropsychology. That is when you compare a single person (often a patient with some brain lesion) against a normative control group, on some normally distributed function - let\u0026rsquo;s say reaction time. What we want to do is to say whether or not this person has a deficit on this function. Naturally, the probability of detecting such a deficit depends on where in the distribution this patient was located prior to the lesion, i.e. the premorbid ability. Look at the expertly drawn picture below for an example:\nAssume a deficit on a this reaction time of ~2 SD caused by some lesion – this means that in this lesioned population, a deficit will only be found 50 % of the time, as can be seen in the above illustration. That is, the power of detecting this deficit will never exceed 50 %. A deficit of 2 σ can never cross the critical value if a patient had a premorbid ability above the mean (σ is here the standard deviation). The problem of premorbid ability generalises to group lesion studies as well, but more in the sense that it increases variability that can cloud inferences. This calls for the need of increasing sample sizes which often is cumbersome and, in some cases, impossible in the lesioned populations. So, what would this lead to in the long run? By running simulations of studies (in hundreds of thousands) one can approximate how such bias would affect the overall estimation given specific parameters, like the size of the deficit and the number of people in the control sample. I will include my code below:\nwin_sim \u0026lt;- function (nsim, deficit, ncon) {\r# Matrix where each row represents observations\r# on a variable from controls plus a patient\rranmat \u0026lt;- matrix(rnorm((ncon + 1) * nsim), nrow = nsim,\rncol = ncon + 1)\r# Induce a deficit on the first obs of each row\r# and compare to the other obs on that row.\r# Save the median of all found deficits.\rmed_def \u0026lt;- median(\rapply(cl, ranmat, 1, function(x) {if (pt(\r((x[1] - def) - mean(x[2:ncon+1]))\r/(sd(x[2:ncon+1])*sqrt(length(x)\r/(length(x) - 1))),\rdf = length(x) - 2)\u0026lt;0.05) {\r(x[1] - def)\r} else { NA } }), na.rm = TRUE)\r# Return the average overestimation\rreturn(- def -med_def) }\r This simulation returns the median overestimation of as many studies as you choose. If you run it for several different parameter combinations (number of controls and size of deficit). In the graph below I have taken the median of 1000 000 simulated single case studies over functional deficits ranging from 1 to 4 standard deviations for 4 different sizes of control groups.\nFigure 1: Median overestimation with Crawford and Howell (1998) method, 10^6^ simulations for every parameter combination.\n\rWhat becomes evident here is that even with a deficit of 2 σ where we would expect a probability of 50 %, we would get a \u0026ldquo;general\u0026rdquo; overestimation of this defict of about 0.6 σ (which is quite alot). This is due to the fact that we cannot detect the people that have a premobrbid ability above the population mean, if not our control sample by chance happens to be drawn from the \u0026ldquo;right side\u0026rdquo; side of the distribution (that is we draw a control sample with high reaction time abilities). This also explains the curious case when we have a small control sample and a small effect size - yielding a lower overestimation, even though we technically should have lower power. It is easier for a small sample to have a mean that differs from the population mean (think again about the IMDB score), meaning that they can be both higher and lower in an equally distributed manner. However, since our simulated patient solely is diagnosed with a deficit if they have a reaction time that is lower than the control sample mean, this will bias the result in favour of when the control sample are drawn from the right side of the distribution - making it easier to detect smaller deficits and hence the overestimation is lowered.\nWhat else is evident is that the difference in overestimation between a control sample size of 15 and that of 70 is marginal. Meaning that single case reearchers would waste their resources collecting data from larger control sample sizes, given that they want to minimise overestimation.\nThis is just one problem that low power yields and in a very specific field. In the next post I will simulate the effects in more general experiments and talk about other issues, such as positive predictive values in fields. However, I hope that by showing you this, it is clear that low power is not only bad because it will become harder for you to obtain significance in your undergrad thesis. It is bad because, in the long run, it can invalidate our field.\n","date":1584403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584456528,"objectID":"4792175168e1d9fff85491b804896b8f","permalink":"/post/the-curious-case-of-the-winner-s-curse/","publishdate":"2020-03-17T00:00:00Z","relpermalink":"/post/the-curious-case-of-the-winner-s-curse/","section":"post","summary":"Key concepts:\n Power = the probability of finding an effect if the effect is true  Have you ever been at a psychology stats lecture listening to your teacher rambling about \u0026ldquo;power\u0026rdquo;, talking about its importance for your experiment to yield a significant result but without really explaining the \u0026ldquo;why\u0026rdquo; of it all?","tags":["Monte Carlo","Simulation","Power","Winner's Curse"],"title":"The Curious Case of the Winner's Curse","type":"post"},{"authors":["Jonathan Ö. Rittmo"],"categories":null,"content":"","date":1580342400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580342400,"objectID":"9923583e57d8142989f6f17fd41262c3","permalink":"/publication/neuropsychdiss/","publishdate":"2020-01-30T00:00:00Z","relpermalink":"/publication/neuropsychdiss/","section":"publication","summary":"The main research questions for this project are which are the main statistical methods used in single-case studies over the last ten years? How do the methods compare and what does their power curves look like? Can a multivariate approach to detecting dissociations be developed and would a power advantage to its univariate counterpart be found? Would such method more reliably measure the underlying contructs we believe to have found? To what extent does the winners curse affect neuropsychology? How would the distribution of corrected effect sizes look? All the above form the basis of the more general question how can we improve the validity of neuropsychology?","tags":["Monte Carlo simulations","Hotteling's T2","Dissociation","Power","Winner's Curse"],"title":"Single Case Neuropsychology: validity and reliability in a power-wise pinioned field (work in progress)","type":"publication"},{"authors":["Jonathan Ö. Rittmo"],"categories":null,"content":"","date":1578787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578787200,"objectID":"96c0d24758bab983dbdb7f55b0783b49","permalink":"/publication/statthesis/","publishdate":"2020-01-12T00:00:00Z","relpermalink":"/publication/statthesis/","section":"publication","summary":"A review of the multivariate analysis of variance (MANOVA) along with its appropriateness under various conditions is presented. In particular, the thesis addresses how different covariance structures plausible in brain imaging, and strengths of the correlations between dependent variables in simulated variable systems affect the power of the analysis in relation to various combinations of effect sizes. The aim was to illuminate these relationships and offer insights in contingencies yielding the highest power, aiding researchers in designing powerful experiments. Simulations revealed that the most improbable conditions yielded the highest power, i.e. when effect sizes differed in size and/or direction, but the correlation was highly positive or when the effect sizes and directions were equal but the correlation highly negative. Similar patterns have been demonstrated before (Cole, Maxwell, Arvey, \u0026 Salas, 1994) but the present thesis offers a generalisation, demonstrating that the effect is present for an arbitrary number of dependent variables and groups. Furthermore, it is shown that multivariate analyses exhibit lower power when effect sizes are small as compared to medium even in situations where 6 variables exhibit a small effect compared to 1 of the variables exhibiting a medium effect. Difficulties in controlling the Type 1 error rate are addressed as well.","tags":["Monte Carlo simulations","Power analysis","MANOVA"],"title":"MANOVA and its contingencies: on the relation between power and correlation - Can the utilisation of multivariate analyses alleviate the power crisis in neuropsychology?","type":"publication"},{"authors":["Jonathan Ö. Rittmo","Rickard Carlsson"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"5266342d02d961078b4ab9e34a4e84bb","permalink":"/publication/cogdiss/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/cogdiss/","section":"publication","summary":"A study using an incomplete repeated measures design was performed to investigate whether a construction of other peoples’ associative mappings (in this case stimulus-outcome mappings) in 33 subjects could occur when observing another person’s facial expression while they perform a conditional discrimination task with differential value-based outcomes, and if such construction could facilitate the subjects’ own learning when afterwards performing the same task. Through this, providing evidence for the use of such constructions as a mechanism aiding individuals engaged in joint activities, including so called joint action. A significantly better performance was observed when such construction was possible in comparison with a condition where it was not, indicating that the construction of others’ associative mappings do occur, at least in conditional discrimination tasks. To further strengthen the indication that better performance was due to construction of the other’s associ- ative mappings and that this construction emerged due to social processing, EEG activity was measured during the task and compared to activity measured during a similar, though non-social, task. Oscillatory differences in EEG between conditions was used as an index of social perception. Differences in power/synchronisation over the alpha, beta and mu band between the conditions were analysed. All bands showed a significant increase in power in the social condition, although mu was expected to show the reversed pattern. Alpha increase may suggest inhibition of the self-perspective and beta increase could be related to processing of emotionally valanced stimuli or use of executive functions – both results may indicate that the social condition was perceived as social.","tags":["EEG","Frequency band analysis","Differential Oucomes Effect","Operant Learning","Forced Choice task","Emotional Contagion"],"title":"Oscillatory Differences in EEG - An index for social processing? A social affective perspective on the associative two-process theory of the differential outcomes effect","type":"publication"}]