---
title: 'Single case neuropsychology: power and multivariate methods <br/> - Investigations in a power-wise pinioned field'
author:
  - name: Jonathan Ã–. Rittmo
    orcid: '0000-0001-5075-0166'
  - name: "Supervised by Robert McIntosh"
affiliation:
    address: School of Philosophy, Psychology and Language Sciences, University of Edinburgh
column_numbers: 3
logoright_name: https&#58;//pbs.twimg.com/profile_images/894558384535502848/vycYwQWf.jpg
logoleft_name: https&#58;//pbs.twimg.com/profile_images/894558384535502848/vycYwQWf.jpg
output: 
  posterdown::posterdown_html:
    self_contained: false
bibliography: poster_refs.bib
primary_colour: "#497a7b"
columnline_style: solid
fig_caption: yes
header-includes:
  - \usepackage{amsmath}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(parallel)
library(cowplot)
```

# Introduction

The reflection of cognitive functions in the brain is often assessed by studying individuals with specific brain damage to evaluate functional impairments and dissociations. Since the distribution of patients with equally specific and focal brain damage usually is scattered, *single cases* are frequently used and compared to controls. This approach is inherently low powered.

How, then, does one design single case experiments and interpret their results while still retaining as much validity as possible? Firstly, knowledge and choice of statistical methods used is as always important. Unfortunately, little work has been done evaluating power of the most frequently used methods. Secondly, multivariate approaches are known to generate higher power than univariate - but the univariate methods used in single case research has to my knowledge not yet been generalised. Lastly, to correctly interpret found effects it is important to adjust them for over-estimation. Such over-estimation is referred to as "the winner's curse" and common in low powered fields. Hence, the extent of this bias is important to be aware of. These three gaps constitute the pillars of this project.

## Objectives

1. Evaluate power for frequently used tests for discovering functional deficits and dissociations.
2. Construct and evaluate multivariate tests for single cases.
3. Evaluate the extent to which single case studies are affected by the winner's curse (see figure \@ref(fig:standard-plot)).
4. Develop R package "singcar" for easy utilisation of the methods. 

# Methods and Preliminary Results

Power evaluation - analytic or numeric solutions?

- Analytic:
    - Elegant and exact.
    - Sometimes difficult or impossible to find.
- Numeric:
    - Approximate, not exact.
    - Sometimes the only alternative.
    
Monte Carlo simulations in particular provide an intuitive understanding of how statistical issues will affect a field "in the long run". As such, this numerical method is especially suited to assess the winner's curse, see example approach below and results figure \@ref(fig:standard-plot).


```{r sim_snippet, echo=TRUE, eval=FALSE}
win_sim_m <- function (nsim, def, ncon) {
  ranmat <- matrix(rnorm((ncon + 1) * nsim), 
                   nrow = nsim,
                   ncol = ncon + 1)
  cl = makeCluster(4, type = "PSOCK")
  on.exit(stopCluster(cl))
  med_def <- median(
    parApply(cl, ranmat, 1, 
             function(x) {if (pt(
                            ((x[1] - def) - mean(x[2:ncon+1]))
                            /(sd(x[2:ncon+1])*sqrt(length(x)
                                              /(length(x) - 1))),
                           df =  length(x) - 2)<0.05) {
                           (x[1] - def)
                         } else { NA } }), 
              na.rm = TRUE)  
  return(- def -med_def) 
}
```

```{r standard-plot, out.width='100%', fig.cap='Median over-estimation with Crawford and Howell (1998) method, 10^6^ simulations for every parameter combination.', fig.align='left', fig.height=4, dev.args = list(type = 'cairo')}
load("C:/Users/jritt/Google Drive/Skola/Edinburgh/Dissertation/R/Simuations - winners curse/wc_def_ncon5-70.RData")

wc <- win_curse_tidy_def %>% filter(ncon == "ncon_5" | ncon == "ncon_15" | ncon == "ncon_25" | ncon == "ncon_70")   

ggplot(wc, aes(x = deficit, y = ave_over_est, color=ncon)) +
  geom_line(size = 0.8) +
  labs(x = "Deficit (in SD)", y = "Average over-estimation (median)") +
  theme_classic() +
  theme(legend.position=c(0.5, 0.98),
        legend.direction = "horizontal") +
  scale_color_brewer(name = "Control sample size",
                       limits = c("ncon_5", "ncon_15", "ncon_25", "ncon_70"),
                       labels = c("5", "15", "25", "70"),
                     palette = 2,
                     direction = -1)
```

One approach to develop a multivariate method for detecting deficits could be to build on the modified t test advocated by @crawford_comparing_1998. This test, seen in eq. \@ref(eq:ttest), replaced the common approach of using z-scores for detecting deficits due to the obvious out performance concerning Type I errors since it does not rely on knowing the population variance.
\begin{equation}
t_{n-1}=\frac{X^*-\overline{X}}{s\sqrt{\frac{n+1}{n}}}
(\#eq:ttest)
\end{equation}
Where $X^*$ is the patient score, $\overline{X}$, $s$ and $n$ the mean, standard deviation and size of the control sample. This is a two sample t test where one of the groups only has one observation, naturally collapsing the pooled standard deviation into the standard deviation of the controls. The multivariate generalisation of a two sample t test is the Hotelling's $t^2$ test:
\begin{equation}
t^2=\frac{n_{x}n_{y}}{n_{x}+n_{y}}(\bar{\pmb {x}}-\bar{\pmb {y}})'{\hat{\pmb{\Sigma}}}^{-1}_{p}(\bar{\pmb {x}}-\bar{\pmb {y}})
(\#eq:hot)
\end{equation}
<!-- \sim T^{2}(p,n_{x}+n_{y}-2) -->
$\bar{\pmb {x}}$ and $\bar{\pmb {y}}$ being the mean *vectors* of the two groups and ${\hat{\pmb{\Sigma}}_{p}}$ the unbiased pooled covariance *matrix* and, if we let $p=no. \: of\: variates$, where
\begin{equation}
\frac {n_{x}+n_{y}-p-1}{(n_{x}+n_{y}-2)p} t^{2} \sim F(p,n_{x}+n_{y}-1-p)
(\#eq:fdist)
\end{equation}
Following the same logic as @crawford_comparing_1998, it should be possible to generalise their method to *p* number of variates as follows:
\begin{equation}
t^2=\frac{n}{n+1}(\pmb{x}^* - \bar{\pmb{x}})'{\hat{\pmb{\Sigma}}}^{-1}(\pmb {x}^* -\bar{\pmb {x}})
(\#eq:singhot)
\end{equation}
Where $\pmb{x}^*$ is a vector of observations on each variate from the individual of interest, $\bar{\pmb{x}}$,  ${\hat{\pmb{\Sigma}}}$ and $n$ the mean vector, covariance matrix and size of the control sample. The result of simulations in figure \@ref(fig:singhot) shows power curves for this method in a scenario with two variates, over the correlation between them. It also shows that in a true null scenario the Type I errors are controlled.

```{r singhot, out.width='100%', fig.cap='Power for single case Hotellings t^2^ over correlation, 10^5^ simulations for every parameter combination. Deficits measured in standard deviations.', fig.align='left', fig.height=4, dev.args = list(type = 'cairo')}
load("C:/Users/jritt/Google Drive/Skola/Edinburgh/Dissertation/R/Simuations - winners curse/singhot_100Ksim_esxy.Rdata")

a <-ggplot(filter(singhot_dat_tidy, esy == 0 | esy ==1), aes(x = r, y = power, color=esx)) +
  geom_line(size = 0.8) +
  labs(x = "Correlation", y = "Power") +
  facet_grid(cols = vars(esy),
             labeller=labeller(esy = c("0"="Deficit Y: 0 (SD)", "1"="Deficit Y: 1 (SD)"))) +
  theme_classic() +
  theme(legend.position=c(0.5, 1.29),
        legend.direction = "horizontal",
        legend.background = element_rect(fill=alpha(0)),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        plot.margin = margin(0.7, 0, 0, 0, "cm"))+
  ylim(0, 1) +
  scale_color_brewer(name = "Deficit X (SD)",
                       limits = c("esxy0", "esxy1", "esxy2", "esxy3"),
                       labels = c("0", "1", "2", "3"),
                    palette = 2,
                    direction = -1)


b <- ggplot(filter(singhot_dat_tidy, esy == 2 | esy == 3), aes(x = r, y = power, color=esx)) +
  geom_line(size = 0.8) +
  labs(x = "Correlation", y = "Power") +
  facet_grid(cols = vars(esy),
             labeller=labeller(esy = c("2"="Deficit Y: 2 (SD)", "3"="Deficit Y: 3 (SD)"))) +
  theme_classic() +
  theme(legend.position="",
        legend.direction = "horizontal",
        legend.background = element_rect(fill=alpha(0)),
        plot.margin = margin(0, 0, 0, 0, "cm")) +
  ylim(0, 1) +
  scale_color_brewer(name = "Deficit X (SD)",
                     limits = c("esxy0", "esxy1", "esxy2", "esxy3"),
                     labels = c("0", "1", "2", "3"),
                     palette = 2,
                     direction = -1)

plot_grid(a, b, nrow = 2)
```

# Next Steps

The above method is a straightforward multivariate generalisation. However, other methods could also be imagined, such as various distance measures or classifying approaches which could be valuable for comparison. For display purposes, this poster has mainly concerned detection of deficits. Neuropsychologist are, however, often interested in *dissociations* for drawing inferences of the functional architecture of the brain. Evaluation of methods for discovering these as well as dissociation analogues to eq. \@ref(eq:singhot) will not be omitted in the thesis. To more easily evaluate tests used in the literature, an R package ("singcar") that includes them are under development. 

### Acknowledgements

Poster created with posterdown [@R-posterdown].

```{r, include=FALSE}
knitr::write_bib(c('ioannidis_why_2005', 'crawford_comparing_1998', 'crawford_testing_2005', 'R-posterdown'), 'poster_refs.bib')
```

# References


