---
title: Primary Concepts of Power Curves
author: Jonathan Rittmo
date: '2020-04-04'
slug: primary-concepts-of-power-curves
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2020-04-04T14:32:54+01:00'
featured: no
output:
  blogdown::html_page:
    dev: "svg"
image:
  caption: 'Image credit: [**Photo by Samantha Sophia on Unsplash**](https://unsplash.com/photos/zFIKq7VEk1o)'
  focal_point: ''
  preview_only: no
projects: []
header-includes:
 - \usepackage{amssymb}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
options(knitr.table.format = "html") 
library(tidyverse)
library(plotly)
library(widgetframe)
```

<!-- This post did not at all become what I thought it would when I started writing it. I was going to write about visualisation of multivariate power curves, but halfway through of explaining what a power curve is I realised that it probably would be better to split the post into two. So here comes the first half! -->

As the image above (probably abused by me) clearly states: "the abuse of power comes as no surprise". Whether intentional or not, ingorance of a priori power calculations have been prominent in psychology and related fields. 

But this is not surprising, because it is difficult to grasp what a power calculation actually is. In this post I will walk through some of the basic calculations behind deriving the power function for a paired samples z-test analytically (that is obtaining an exact mathematical function) and how you can calculate power for an independent two sample t-test, by simulations in R. Hopefully this will show that the basic concepts of power calculations and power curves actually are quite easy! So here goes:

The power function for a hypothesis test is defined as the following:

$$power(\theta) =\mathbb{P}[\text{reject} \: H_0 \: \text{for a given} \: \theta]$$

Where $\theta$ is a given effect size, $\mathbb{P}$ probability and $H_0$ the null hypothesis. In more general terms it is defined (the vertical bar is read "given"):

$$power =\mathbb{P}[\text{reject} \: H_0\:  | \: H_1 \: \text{is true}]$$ 

Say that we want to test whether a medication has had some effect on a sample (we measure a continuous dependent variable on the same individuals before and after treatment) and we assume that we know the population standard deviation. If $\overline{D}$ is the average difference between the observations, for each individual then $\overline{D} \sim N(\mu, \sigma^2)$, meaning that $\overline{D}$ follows the normal distribution with some population mean ($\mu$) and variance ($\sigma^2$). Standardising $\overline{D}$ by centering it around 0 (since we are talking about a difference it is already centered around 0 though) and dividing by the standard error we would have an entitiy that follows the standard normal distribution:

$$\frac{\overline{D}-0}{{{\sigma}_D}/{\sqrt{n}}}\sim Z$$

Where $\overline{D}$ is the average of the differences between the two measurement time points and ${\sigma}_D$ the population standard deviation and hence ${{\sigma}_D}/{\sqrt{n}}$ the standard error and thus, if we input real values we would get a z-value. The null and alternative hypotheses for this test are $H_0:\: \mu_D \: = \: 0$ and $H_1:\: \mu_D \: \neq \: 0$ where $\mu_D$ is estimated by $\overline{D}_n$.

If we set $\alpha\:=\:0.05$ (i.e. $\mathbb{P}$[reject $H_0$ | $H_0$ is true]$\:=\:0.05$) our critical z-value would be $z_{(0.05/2) }=$ $\pm$ `r abs(qnorm(0.025)%>%round(4))`. If you are unfamilliar with critical values, have a look at the density plot below.

```{r zdist, echo=F}

Z_dist <- data.frame(Z=seq(-4, 4, length.out = 1000),
                     p=dnorm(seq(-4, 4, length.out = 1000))) %>%
      mutate(variable = case_when(
      (Z <= qnorm(0.025)) ~ "lower",
      (Z>=qnorm(0.025, lower.tail = F)) ~ "upper",
      TRUE ~ NA_character_))

ggplot(Z_dist, aes(x = Z,
           y = p)) +
  geom_line() +
  geom_vline(xintercept = qnorm(0.025), color='lightblue', linetype = "dashed") +
  geom_vline(xintercept = qnorm(0.025, lower.tail = F), color='lightblue', linetype = "dashed") +
  geom_area(data = filter(Z_dist, variable == 'lower'), fill = 'lightblue') +
  geom_area(data = filter(Z_dist, variable == 'upper'), fill = 'lightblue') +
  annotate("text", x = 3.2, y = 0.4, label = "Upper cutoff - 0.025%") +
  annotate("text", x = -3.2, y = 0.4, label = "Lower cutoff - 0.025%") +
  ylab("Density") +
  theme_classic()

```


All observed z-values that fall above the upper or below the lower critical cutoff values are considered significant at the $\alpha \: =  \: 0.05$ level. Now suppose that $\mu_D \: \neq \: 0$, i.e. $\mu_D =$ $\theta$. Then we get the power by calculating:

$$\mathbb{P}[1.96 < Z < -1.96 \; | \; \mu_D =\theta]$$

Since $\frac{\overline{D}_n-0}{{{\sigma}_D}/{\sqrt{n}}}\sim Z$ we'll write:

$$\mathbb{P}[1.96 < \frac{\overline{D}_{n}-0}{{{\sigma}_D}/{\sqrt{n}}} < -1.96 \; | \; \mu_D =\theta]$$

We'll subtract and add $\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}$, i.e. a neutral operation:

$$\mathbb{P}[1.96 < \frac{\overline{D}_{n}\:-\:0\:-\:\theta\:+\:\theta}{{{\sigma}_D}/{\sqrt{n}}} < -1.96 \; | \; \mu_D =\theta]$$

And then move $+\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}$ to the other sides of the inequalities (be careful with the signs!):

$$\mathbb{P}[1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}< \frac{\overline{D}_{n}\:-\:\theta}{{{\sigma}_D}/{\sqrt{n}}} < -1.96 -\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}} \; | \; \mu_D =\theta]$$ 

Then we have:

$$power(\theta) = \mathbb{P}[1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}< Z < -1.96 -\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}]$$

Which is the same as:

$$power(\theta) = \mathbb{P}[Z < -1.96 -\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}] + \mathbb{P}[Z > 1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}]$$

Note that 

$$\mathbb{P}[Z > 1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}]$$

also can be written

$$1 \: - \: \mathbb{P}[Z < 1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}]$$

So now we can write our power function for this test:

$$power(\theta) = \mathbb{P}[Z < -1.96 +\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}] + 1 \: - \: \mathbb{P}[Z < 1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}]$$

The notation $\mathbb{P}[Z < z]$ is usually written $N(z)$ and denotes the *area* under the cumulative distribution function up til the value of $z$. 

This became much more technical than what I imagined when I started out with this post. I might have to push my amateur adventures in 3D visualisation to the next post! Anyway, now we have:

$$power(\theta) = N(-1.96 +\frac{\theta}{{{\sigma}_D}/{\sqrt{n}}}) + 1 - N(1.96\: - \frac{\theta}{{{\sigma}_D}/{\sqrt{n}}})$$

If we assume $\sigma_D=1$ and some sample size this will give us a single value for each $\theta$ we put in. We can easily calculate this in R and plot the power against possible values of $\theta$ &ndash; this is what is known as a power curve. If we instead are interested in seeing how sample size affect power, we hold $\theta$ fixed and vary *n*. Below I show the code for for how to calculate and draw the power curve via this function in R for a specified sample size of $n=25$. In the interactive graph below though, you can adjust the sample size yourself and see how that affects power. 

```{r powercode, echo=T, eval=F}
power_data <- data.frame(theta = seq(from = -1.2, to = 1.2, length.out = 100)) %>% # Create a data frame with a variabl of various theta
  mutate(power = pnorm(-1.96-theta*sqrt(25)) + (1-pnorm(1.96-theta*sqrt(25)))) # Add a variable that for each theta gives us power, according to the power function above


ggplot(power_data, aes(x= theta, y=power)) + # Plot power over theta
  geom_line(size=1) +
  geom_hline(yintercept = 0.05, linetype ="dashed", color = "darkblue") +
  geom_vline(xintercept = 0) +
  xlab(expression(paste(theta))) +
  ylab("Power") +
  annotate(geom = "text", x = 0.3, y = 0.08,
           label = "alpha",
           parse = T,
           color = "darkblue",
           size = 5) +
  theme_classic()


```


```{r powercurve, echo=F, warning=FALSE}

power_data <- data.frame(theta = seq(from = -1.2, to = 1.2, length.out = 100)) %>%
  mutate(power = pnorm(-1.96-theta*sqrt(5)) + (1-pnorm(1.96-theta*sqrt(5)))) %>%
  mutate(n = 5)

for (i in seq(10, 250, by = 5)) {
 pd <- data.frame(theta = seq(from = -1.2, to = 1.2, length.out = 100)) %>%
       mutate(power = pnorm(-1.96-theta*sqrt(i)) + (1-pnorm(1.96-theta*sqrt(i)))) %>%
       mutate(n = i)
 
 power_data <- rbind(power_data, pd)
}



p <- ggplot(power_data, aes(x = theta, y = power)) +
  geom_line(aes(frame = n), size = 1) +  
  geom_hline(yintercept = 0.05, linetype ="dashed", color = "darkblue") +
  geom_vline(xintercept = 0) +
  xlab("&#120637;") +
  ylab("Power") +
  annotate(geom = "text", x = 1.3, y = 0.1,
           label = "&#9082;",
           parse = T,
           color = "darkblue",
           size = 8) +
  theme_classic()

fig <- ggplotly(p)

frameWidget(fig)

```

As the true effect becomes bigger in absolute values, power goes to 100%. Optimally one would want a power function that wasn't curved but instead always rejected $H_0$ whenever it was false, how small the effect may be. I.e. we would want a power function that fulfills:

\begin{equation} 
  power =\mathbb{P}[\text{reject} \: H_0 \: | \: \text{any} \: \theta \neq 0] = 1
  (\#eq:1)
\end{equation} 

**and**

\begin{equation} 
  power =\mathbb{P}[\text{reject} \: H_0 \: | \: \theta = 0] = 0
  (\#eq:2)
\end{equation} 


However, tests that fufill these are in practice difficult (if not impossible) to find and we must compromise. For example, equation \@ref(eq:2) tells us that we do not ever want to reject the null hypothesis if it is in fact true. The probability of rejecting the null when the null is true is what we usually refer to as our &alpha;-level which we most commonly set to $0.05$ in psychology, i.e. it is the probbility of a Type I error (falsely rejecting the null hypothesis). This compromise can be seen in the power curve above. Additionally, the smaller &theta; gets, the more difficult the effect is to detect, because the areas under the distribution function shrink.

When we now know what a power curve is and how it can be derived analytically for a given test, it might be worth mentioning how one can approximate power numerically with simulations. I am not going to use the paired example as I did above, because that requires us to assume some correlation between the measurements. I am going to show the easiest example of simulating power for a t-test between two independent groups, i.e. we assume independence between observations. 

The basic concept behind it is fairly simple, we draw one sample from a normal distributions with a mean of 0 and we draw one sample from a normal distribution with a mean of &theta;. We then run a t-test to compare the mean differences between the samples to see obtain a p-value. We do this a couple of times and save the p-values for every test. Then we divide the number of significant p-values (p-values < 0.05 in this case) on the total number of tests run and the ratio that we obtain is the power. That is, the power is the number of significant tests, divided by the total number of simulations. The more simulations run, the closer our numerical solution will be to the analytical. Look at the code below.

```{r powsim, echo=TRUE}

power_sim <- function (nsim, theta, n1, n2) {

  sim_mat <- cbind(matrix(rnorm(n1*nsim, mean = theta, sd=1), 
                   nrow = nsim,
                   ncol = n1),
                   matrix(rnorm(n2*nsim, mean = 0, sd=1), 
                   nrow = nsim,
                   ncol = n2))
  
  p_val <- apply(X = sim_mat, MARGIN =  1, 
                        function(x) t.test(x = x[1:n1], y = x[(n1+1):(n1+n2)],
                                           alternative = "two.sided",
                                           paired = FALSE)[["p.value"]]
                 )
  
  power <- sum(p_val < 0.05)/length(p_val)
  
  return(power)
}


```


Say that we want to see what the power curve would look like over different values of &theta;, as we did above. We then need to run the simulations for every specified value of &theta;. This is most easily done in a loop, but you can think of it as if we are running the function above for every &theta; that are of interest, for example like this:

```{r echo=T, eval=F}
power_sim(nsim = 50, theta = 0.5 , n1 = 25, n2 = 25)

power_sim(nsim = 50, theta = 1.0 , n1 = 25, n2 = 25)

power_sim(nsim = 50, theta = 1.5 , n1 = 25, n2 = 25)
```

Doing it in a loop however requires a lot less code, so we specify a &theta; vector, with every value that we are interested in and iteratively run the power simulation function we created for each of those values. The code below does this and then plot the obtained power values over &theta; for 50 simulations.

```{r echo=T}

theta <- seq(from = -1.2, to = 1.2, length.out = 50) # Here we specify 50 different values of theta

power <- 0 # We need to define an object outside of the loop to be able to iteratively save the power calculations

for (i in 1:length(theta)) { # This is the loop, which runs the code below and saves the power iteratively
  power[i] <- power_sim(nsim = 50, theta[i], n1 = 25, n2 = 25)
}


power_data <- as.data.frame(cbind(power, theta)) # Putting the theta vector and power vector into a data frame 


ggplot(power_data, aes(x= theta, y=power)) +  # Plotting power over theta, just as we did analytically
  geom_line(size=1) +
  geom_hline(yintercept = 0.05, linetype ="dashed", color = "darkblue") +
  geom_vline(xintercept = 0) +
  xlab(expression(paste(theta))) +
  ylab("Power") +
  annotate(geom = "text", x = 0.3, y = 0.08, 
           label = "alpha", 
           parse = T, 
           color = "darkblue", 
           size = 5) +
  theme_classic()


```

As can be seen, the general pattern is similar to the analytical solution. But since we only ran 50 simulations for every parameter combination we won't get very precise estimations. So let's amp it up to 10,000 simulations:

```{r echo=T, eval=F}

theta <- seq(from = -1.2, to = 1.2, length.out = 50) 

power <- 0 

for (i in 1:length(theta)) { 
  power[i] <- power_sim(nsim = 10000, theta[i], n1 = 25, n2 = 25)
}


power_data <- as.data.frame(cbind(power, theta)) 


ggplot(power_data, aes(x= theta, y=power)) +  # Plotting power over theta, just as we did analytically
  geom_line(size=1) +
  geom_hline(yintercept = 0.05, linetype ="dashed", color = "darkblue") +
  geom_vline(xintercept = 0) +
  xlab(expression(paste(theta))) +
  ylab("Power") +
  annotate(geom = "text", x = 0.3, y = 0.08, 
           label = "alpha", 
           parse = T, 
           color = "darkblue", 
           size = 5) +
  theme_classic()

```

```{r echo=F}
load("10000_pwr.Rdata")

ggplot(power_data, aes(x= theta, y=power)) +  # Plotting power over theta, just as we did analytically
  geom_line(size=1) +
  geom_hline(yintercept = 0.05, linetype ="dashed", color = "darkblue") +
  geom_vline(xintercept = 0) +
  xlab(expression(paste(theta))) +
  ylab("Power") +
  annotate(geom = "text", x = 0.3, y = 0.08, 
           label = "alpha", 
           parse = T, 
           color = "darkblue", 
           size = 5) +
  theme_classic()
```

Now, this looks more like it! This approach is a useful tool for a priori power calculations when an anlytical solution is too difficult (sometimes it's even impossible) to find. But naturally, when our designs get more complex it will be more difficult to calculate power either analytically or with simulations. But I hope that this post at least showed that you don't have to be a mathematician to start out with your a priori power calcs!

Next up: amateur adventures in 3D visualisation of multivariable/variate power curves!

<!-- P.S. If you want to know how tripling the sample size would effect the curve - have a look below: -->

```{r echo=F}
# load("pwr_10k_n150.Rdata")
# 
# 
# ggplot(power_data, aes(x= theta, y=power)) +  # Plotting power over theta, just as we did analytically
#   geom_line(size=1) +
#   geom_hline(yintercept = 0.05, linetype ="dashed", color = "darkblue") +
#   geom_vline(xintercept = 0) +
#   xlab(expression(paste(theta))) +
#   ylab("Power") +
#   ggtitle("n1 = n2 = 75") +
#   annotate(geom = "text", x = 0.3, y = 0.08, 
#            label = "alpha", 
#            parse = T, 
#            color = "darkblue", 
#            size = 5) +
#   theme_classic()

```

